---
title: 'OHI GoC 2024: Sea Surface Temperature (SST)'
author: "*Compiled on `r date()` by `r Sys.info()['user']`*"
format:
  html:
    code-fold: show
    toc: true
    toc-depth: 3
    toc-float: true
    number-sections: false
    theme: cerulean
    highlight-style: haddock
  pdf:
    toc: true
editor: 
  markdown:
    wrap: sentence
  chunk_output_type: inline
editor_options: 
  chunk_output_type: console
---

# Summary

This is a work in progress, aiming to provide a pressure layer of Sea Surface Temperature (SST) for the Gulf of California. We will begin by visualizing the data and determining the yearly average temperature, then progress to a weekly average temperature. We can use the standard deviation to define an anomaly, as was done for OHI global.  The frequency of anomalies in the last 5 years compared to the baseline SST anomalies from historical data will give an idea of how much change has occurred.  Rescaling was done using the “99.99th quantile of raster values from all years”, which we can later evaluate if it should be the same here ([sea-surface-temperature](https://ohi-science.org/ohi-methods/data-layers/data-layer-descriptions.html#sea-surface-temperature)). 

------------------------------------------------------------------------

# Updates

Data was downloaded from CEDA on January 16, 2025.

------------------------------------------------------------------------

# Data Source

**Reference**: Good, S.A.; Embury, O. (2024): ESA Sea Surface Temperature Climate Change Initiative (SST_cci): Level 4 Analysis product, version 3.0. NERC EDS Centre for Environmental Data Analysis, 09 April 2024. doi:10.5285/4a9654136a7148e39b7feb56f8bb02d2. https://dx.doi.org/10.5285/4a9654136a7148e39b7feb56f8bb02d2

**Downloaded**: January 16, 2025

**Description**: ESA Sea Surface Temperature Climate Change Initiative (SST_cci): Level 4 Analysis product, version 3.0

**Time range**: 1980 - ongoing, updated daily at one month behind present

**Format**: NetCDF

**File location**: `Mazu:/home/shares/ohi/OHI_GOC/_raw_data/CEDA_CDRv3.0/19800101120000-ESACCI-L4_GHRSST-SSTdepth-OSTIA-GLOB_CDR3.0-v02.0-fv01.0.nc`

------------------------------------------------------------------------

# Methods

1.  Set-up source and file paths

2.  Download data needed

prs_oa:

<!-- 3.  Split the global OA MultiLayer NetCDF into its individual raster layers, which are by month. -->

<!-- -   This would be saved in Mazu, within `/home/shares/ohi/git-annex/globalprep/prs_oa/v2024/int/oa_monthly_rasters` -->

<!-- 4.  Raster calculations for historical and new data -->

<!-- -   Create a raster of the average historical values by making a `terra` RasterBrick and calculate the average over the reference years (1985 - 2000) -->
<!--     -   Save within `/home/shares/ohi/git-annex/globalprep/prs_oa/v2024/int` -->
<!-- -   Create annual mean rasters for the new data by stacking the monthly rasters by year and using `raster::calc` to calculate the mean for that year. -->
<!--     -   Save within `/home/shares/ohi/git-annex/globalprep/prs_oa/v2024/int/oa_annual_mean` -->

<!-- 5.  Rescale each annual raster between 0 to 1 using the historical average data as a reference -- v2024 updated the function -->

<!-- 6.  Project, resample, and check the extent of the new data, historical ref data, and zones raster from OHI -->

<!-- 7.  Calculate Zonal Statistics using the "mean" between the zones raster and the rescaled annual rasters for each region. -->
<!--     Finish by saving the dataframe within `/home/lecuona/OHI_Intro/ohiprep_v2024/globalprep/prs_oa/v2024/output`. -->

prs_slr:

<!-- - Clips all monthly rasters to the coast using a 3 nautical mile offshore buffer -->
<!-- - Calculates annual mean sea level anomaly rasters from monthly data -->
<!-- - Rescales values from 0 to 1 using the reference point -->
<!-- - Sets to zero all negative values, indicating decreases in mean sea level -->
<!-- - Resamples raster to ~ 1km^2^ and reproject to Molleweide -->
    
------------------------------------------------------------------------

# Setup

```{r}
library(raster)
library(terra)
library(readr)
library(magrittr)
library(mregions2)
library(mapview)
library(sf)
library(tidyverse)
library(lwgeom)
library(here)
library(leaflet.extras2)
library(fasterize)
library(tictoc)
library(kableExtra)
library(purrr)
library(foreach)
library(doParallel)
library(ncdf4)
library(plotly)
library(fs) # for file path handling

# ---- sources! ----
# source(here("workflow", "R", "common.R")) # file creates objects to process data
## set the mazu and neptune data_edit share based on operating system
dir_M             <- c('Windows' = '//mazu.nceas.ucsb.edu/ohi',
                       'Darwin'  = '/Volumes/ohi',    ### connect (cmd-K) to smb://mazu/ohi
                       'Linux'   = '/home/shares/ohi')[[ Sys.info()[['sysname']] ]]

# ---- set year and file path info ----
current_year <- 2025 # Update this in the future!!
version_year <- paste0("v",current_year)
data_dir_version_year <- paste0("d", current_year)

# ---- data directories ----

# raw data directory (on Mazu)
raw_data_dir <- here::here(dir_M, "OHI_GOC", "_raw_data")

# CEDA raw data directory
ceda_dir <- here(raw_data_dir, "CEDA_CDRv3.0", data_dir_version_year)

# output data dir for intermediate data products
int_dir <- here(ceda_dir, "int")
# dir.create(int_dir) # to create the path on Mazu if it has not already been done

# final output dir
output_dir <- here("_pressures", "prs_sst", version_year, "output")

# spatial data for GoC
goc_spatial <- here("spatial")

# set colors
# cols = rev(colorRampPalette(brewer.pal(9, 'Spectral'))(255)) # rainbow color scheme

# this CRS might be better for visualization, explore.
gulf_crs <- "+proj=aea +lat_1=23 +lat_2=30 +lat_0=25 +lon_0=-110 +datum=WGS84 +units=m +no_defs"

# ---------- ecoregion border -----------
goc_ecoregion_border_shp <- st_read(here(goc_spatial, "GoC_polygon_ecoregion_eqarea.shp")) # polygon, proj crs: NAD83 / Conus Albers
```


# Download the data

```{r}
# use wget2 to download all data
base_url <- "https://dap.ceda.ac.uk/neodc/eocis/data/global_and_regional/sea_surface_temperature/CDR_v3/Analysis/L4/v3.0.1"
start_year <- 2023
end_year <- 2023

# set up parallel processing for faster download
cl <- 10
registerDoParallel(cl) # creates a set of copies of R running in parallel and communicating over sockets

# function to generate URLs for each year
download_by_year <- function(year) {
  
  urls <- c() # combine all urls into a list
  
  for (month in 1:12) {
    days_in_month <- if (month == 2) 28 else if (month %in% c(4, 6, 9, 11)) 30 else 31
    for (day in 1:days_in_month) {
      url <- sprintf("%s/%04d/%02d/%02d/%04d%02d%02d120000-ESACCI-L4_GHRSST-SSTdepth-OSTIA-GLOB_CDR3.0-v02.0-fv01.0.nc", 
                     base_url, year, month, day, year, month, day)
      urls <- c(urls, url) # combine that list after every "url" is made
    }
  }

# download the files for each year
  output_dir <- sprintf("/home/shares/ohi/OHI_GOC/_raw_data/CEDA_CDRv3.0/d2025/%04d", year) # mazu output directory
  
  dir.create(output_dir, showWarnings = FALSE, recursive = TRUE) # create the dir if it has not been created yet for that year
  
  wget_command <- sprintf("wget -N -P %s -i -", output_dir) # defining where wget needs to download the data into, and how: -N provides a timestamp so if the download fails, then if a file was halfway downloaded/corrupted it can be determined.  -P %s defines the filepath as output_dir, and -i - tells wget to read the list of urls we made earlier in the function.
  
  system(wget_command, input = paste(urls, collapse = "\n")) # this actually calls wget to work
  
  return(length(urls)) # gives us progress tracking!
  
}

# use the function to process each year in parallel for speed
tic()
results <- foreach(year = start_year:end_year, .combine = c) %dopar% {
  
  download_by_year(year)
  
}
toc() # v2025: took 276.088 sec elapsed for one year (ex: 2014 to 2015)

# double check how many files were downloaded
total_downloaded <- sum(results)
total_downloaded # 730, great! That is for 2 years.  I will work with the first two years to write the scripts to limit processing time, then return back and download all the other years.  It is estimated to take ~5 minutes for a year's worth of data, using 10 cores.

# best way to do it is to go two years at a time (ex: start = 2014, end = 2015; the next download would be start = 2016 to end = 2017).  It is fastest that way.

#### check that the files match the correct dates: restrict the start year and end year to just two years (eg. 1980 - 1981) and see if you get 730 files with names that match the correct the year, month, date is correct as the link in https://dap.ceda.ac.uk/neodc/eocis/data/global_and_regional/sea_surface_temperature/CDR_v3/Analysis/L4/v3.0.1/
```



# Processing Data

## Exploration

Looking at a singular NetCDF file to understand the number of layers

```{r}
# read in NetCDF file downloaded from CEDA
sst_raw <- terra::rast(here(ceda_dir, "1980", "19800101120000-ESACCI-L4_GHRSST-SSTdepth-OSTIA-GLOB_CDR3.0-v02.0-fv01.0.nc")) # S4 class SpatRaster
crs(sst_raw) # not in Albers Eq Area EPSG:5070, will need to be reprojected
plot(sst_raw) # right now all 4 variables are present, each as a separate layer.  However, I want to have it so that there should be a yearly layer for the sea surface temperature. 

# extract the analysed_sst only (perhaps add uncertainty later?)
sst_layer <- sst_raw[["analysed_sst"]]
plot(sst_layer) # now I only see the sst, great!

# split the sst_layer by year using the `terra` package, making it a SpatRasterDataset object
sst_yearly <- terra::split(sst_layer, "years") # now it is a list of 1
# sst_names <- lapply(sst_yearly, magrittr::extract, 1) # using magrittr::extract() instead of '[[]]'
sst_list <- as.list(sst_yearly)

num_layers <- terra::nlyr(sst_layer)
print(num_layers) # just 1, as expected, since we extracted SST

# going line by line of function used for historical and current data to ensure it works properly

# ---------------------------
# how to isolate the analysed SST in one line
analysed_sst <- terra::rast(here(ceda_dir, "1980", "19800101120000-ESACCI-L4_GHRSST-SSTdepth-OSTIA-GLOB_CDR3.0-v02.0-fv01.0.nc"))[["analysed_sst"]]

# reprojecting and clipping
# first, get the CRS string from the ecoregion polygon
target_crs <- st_crs(goc_ecoregion_border_shp)$wkt

# then, reproject the raster using terra
analysed_sst_proj <- terra::project(analysed_sst, target_crs)

# clip the raster and mask in the same line
analyzed_sst_clip <- terra::crop(analysed_sst_proj, goc_ecoregion_border_shp, mask = TRUE)
plot(analyzed_sst_clip) # yay, looks great!

# -----------------------------
# FASTER PROCESSING ORDER:
sst_og_crs <- st_crs(analysed_sst)$wkt
goc_ecoregion_border_sst_proj <- sf::st_transform(goc_ecoregion_border_shp, sst_og_crs)

print(st_bbox(goc_ecoregion_border_sst_proj))
print(ext(analysed_sst))

## clip then reproject to save on processing time
analyzed_sst_clip_first <- terra::crop(analysed_sst, goc_ecoregion_border_sst_proj)
analyzed_sst_reproject_after <- terra::project(analyzed_sst_clip_first, target_crs)
analyzed_sst_mask_last <- terra::mask(analyzed_sst_reproject_after, goc_ecoregion_border_shp)
plot(analyzed_sst_mask_last)
```

## Historical Baseline data (as of now, 1980 - 1990)

Methods for processing:

- Make a list of all files for each year
- Select only the "analysed_sst" variable
- Reproject each daily raster to the same CRS as GoC_polygon_ecoregion_eqarea.shp polygon (NAD83 albers eq area)
- Crop and mask to ensure only SST within the GoC ecoregion boundary is being processed
- Take the mean of all layers for the year to determine the yearly average (in a single layer raster)
- Do this for all years, then combine the single layer rasters into one multilayer SpatRaster, for easy visualization of all years


```{r}
# defining which years to evaluate and process
hist_start_year <- 1987
hist_end_year <- 1990

# bringing this from Setup for ease
goc_ecoregion_border_shp <- st_read(here(goc_spatial, "GoC_polygon_ecoregion_eqarea.shp")) # polygon, proj crs: NAD83 / Conus Albers

# define the target crs for all downloaded rasters
target_crs <- st_crs(goc_ecoregion_border_shp)$wkt

# define where each yearly avg raster is saved: historical in_dir in mazu
hist_int_dir <- here(int_dir, "historical_int")
daily_int_dir <- here(hist_int_dir, "daily_int")

# enable terra parallelization
terraOptions(threads = 60, progress = 3, todisk = TRUE, memfrac = 0.5)

process_sst_daily <- function(year, goc_ecoregion_border_sst_proj, goc_ecoregion_border_shp) {
  
  # make a list of all NetCDF files for each year
  files <- fs::dir_ls( # use the `fs` package because it allows us to list files in a more intuitive way
    path = here::here(ceda_dir, as.character(year)),
    glob = "*.nc"
  )
  
  # read all files into a list, selecting only the "analysed_sst" variable
  daily_rasters <- lapply(files, function(f) { # for each iteration, the current file path is passed as the argument `f` to the anonymous function! so it will do this for each daily raster within each year
    
    r <- terra::rast(f)[["analysed_sst"]] # read in each individual .nc file for the year, isolate the analysed_sst layer
    
    r <- terra::crop(r, goc_ecoregion_border_sst_proj, overwrite = TRUE) # makes the maximum extent of the daily raster limited to the goc ecoregion boundary polygon
    
    # reproject the daily raster to match the ecoregion boundary's CRS
    r <- terra::project(r, target_crs) # NAD93, albers eq area
    
    # clip the daily raster to the GoC ecoregion boundary (using crop() and mask(), see <https://r.geocompx.org/raster-vector> for further explanation)
    
    r <- terra::mask(r, goc_ecoregion_border_shp) # replaces all values outside of the NAD83 ecoregion boundary area to NA
    
    # save the daily raster after it has been processed for better progress tracking
    date <- format(as.Date(terra::time(r)), "%Y_%m_%d")
    daily_out_file <- file.path(daily_int_dir, paste0("daily_proj_crop_sst_", date, ".tif"))
    terra::writeRaster(r, filename = daily_out_file, overwrite = TRUE)
    cat(sprintf("Finished processing %s daily raster\n", date))
    
    return(r) # return the reprojected and clipped daily raster
    
  })
  
  # convert all daily rasters for the year to a multilayer SpatRaster
  sst_stack <- terra::rast(daily_rasters) # just reading in a list will make it a multilayer SpatRaster, where each raster in the list becomes a layer
  
  # calculate the average SST for the year
  yearly_avg <- app(sst_stack, fun = mean, na.rm = TRUE) # terra::app applies a function to all layers
  
  # save the processed yearly raster
  out_file <- here(hist_int_dir, paste0("hist_sst_yearly_avg_", year, ".tif"))
  terra::writeRaster(yearly_avg, filename = out_file, overwrite = TRUE)
  
  return(yearly_avg) # returns a single layer raster where each pixel is the average of the SST over all days in the year, saves in Mazu
}

# set up parallel processing using `doParallel`
cl <- 60
registerDoParallel(cl)

# use the function with foreach for faster processing time!
tic()
hist_yearly_averages <- foreach(year = hist_start_year:hist_end_year) %dopar% {
  
  process_sst_daily(year, goc_ecoregion_border_sst_proj, goc_ecoregion_border_shp)
  
}
toc() # for 1980, it took 860.973 sec elapsed
```


Bring all the daily rasters that have been reprojected and cropped/masked into a multilayer SpatRaster

```{r}
# bring all yearly averages into a single multilayer raster
hist_yearly_sst_list <- fs::dir_ls( # use the `fs` package because it allows us to list files in a more intuitive way
    path = here::here(hist_int_dir),
    glob = "*.tif"
  )

hist_yearly_averages <- terra::rast(hist_yearly_sst_list)
plot(hist_yearly_averages)

# save it to int_dir in mazu: "/home/shares/ohi/OHI_GOC/_raw_data/CEDA_CDRv3.0/d2025/int"
# writeRaster(hist_yearly_averages, filename = here(int_dir, "historical_yearly_sst_averages_goc_eq_area_1980_1990.tif"), overwrite = TRUE)
```

Make an animation of average SST from 1980 to 1990:

```{r}
hist_animation <- terra::animate(hist_yearly_averages, pause=0.25, maxcell=50000, n=1)
```


Plot the yearly average to verify

```{r}
test_sst <- terra::rast(here(daily_int_dir, "daily_proj_crop_sst_1982_05_23.tif"))
plot(test_sst)

year_avg_1980 <- st_read(here(hist_int_dir, "hist_sst_yearly_avg_1981.tif"))
plot(hist_final_raster)
```



## Current GoC data ( for now, 2014 - 2021)

```{r}
# defining which years to evaluate and process
current_start_year <- 2014
current_end_year <- 2021

# bringing this from Setup for ease
goc_ecoregion_border_shp <- st_read(here(goc_spatial, "GoC_polygon_ecoregion_eqarea.shp")) # polygon, proj crs: NAD83 / Conus Albers

# define the target crs for all downloaded rasters
target_crs <- st_crs(goc_ecoregion_border_shp)$wkt

# define where each yearly avg raster is saved: historical in_dir in mazu
current_int_dir <- here(int_dir, "current_int")

process_year <- function(year, goc_ecoregion_border_shp) {
  
  # make a list of all NetCDF files for each year
  files <- fs::dir_ls( # use the `fs` package because it allows us to list files in a more intuitive way
    path = here::here(ceda_dir, as.character(year)),
    glob = "*.nc"
  )
  
  # read all files into a list, selecting only the "analysed_sst" variable
  daily_rasters <- lapply(files, function(f) { # for each iteration, the current file path is passed as the argument `f` to the anonymous function! so it will do this for each daily raster within each year
    
    r <- terra::rast(f)[["analysed_sst"]] # read in each individual .nc file for the year, isolate the analysed_sst layer
    
    # reproject the daily raster to match the ecoregion boundary's CRS
    r <- terra::project(r, target_crs) # NAD93, albers eq area
    
    # clip the daily raster to the GoC ecoregion boundary (using crop() and mask(), see <https://r.geocompx.org/raster-vector> for further explanation)
    r <- terra::crop(r, goc_ecoregion_border_shp, mask = TRUE) # makes the maximum extent of the daily raster limited to the goc ecoregion boundary polygon
    # r <- terra::mask(r, goc_ecoregion_border_shp) # replaces all values outside of the area to NA
    
    return(r) # return the reprojected and clipped daily raster
    
  })
  
  # convert all daily rasters for the year to a multilayer SpatRaster
  sst_stack <- terra::rast(daily_rasters) # just reading in a list will make it a multilayer SpatRaster, where each raster in the list becomes a layer
  
  # calculate the average SST for the year
  yearly_avg <- app(sst_stack, fun = mean, na.rm = TRUE) # terra::app applies a function to all layers
  
  return(file.path(current_int_dir, paste0("hist_sst_avg_", year, ".tif"))) # returns a single layer raster where each pixel is the average of the SST over all days in the year, saves in Mazu
}

# set up parallel processing using `doParallel`
cl <- 10
registerDoParallel(cl)

# use the function with foreach for faster processing time!
tic()
current_yearly_averages <- foreach(year = current_start_year:current_end_year) %dopar% {
  
  process_year(year, goc_ecoregion_border_shp)
  
}
toc()

# bring all yearly averages into a single multilayer raster
current_final_raster <- terra::rast(current_yearly_averages)

# save it to int_dir in mazu: "/home/shares/ohi/OHI_GOC/_raw_data/CEDA_CDRv3.0/d2025/int"
writeRaster(current_final_raster, filename = here(int_dir, "current_yearly_sst_averages_goc_eq_area_2014_2021.tif"), overwrite = TRUE)
```



