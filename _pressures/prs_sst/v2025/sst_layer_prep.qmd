---
title: 'OHI GoC 2024: Sea Surface Temperature (SST)'
author: "*Compiled on `r date()` by `r Sys.info()['user']`*"
format:
  html:
    code-fold: show
    toc: true
    toc-depth: 3
    toc-float: true
    number-sections: false
    theme: cerulean
    highlight-style: haddock
  pdf:
    toc: true
editor: 
  markdown:
    wrap: sentence
  chunk_output_type: inline
editor_options: 
  chunk_output_type: inline
---

# Summary

This is a work in progress, aiming to provide a pressure layer of Sea Surface Temperature (SST) for the Gulf of California. We will begin by visualizing the data and determining the yearly average temperature, then progress to a weekly average temperature. We can use the standard deviation to define an anomaly, as was done for OHI global.  The frequency of anomalies in the last 5 years compared to the baseline SST anomalies from historical data will give an idea of how much change has occurred.  Rescaling was done using the “99.99th quantile of raster values from all years”, which we can later evaluate if it should be the same here ([sea-surface-temperature](https://ohi-science.org/ohi-methods/data-layers/data-layer-descriptions.html#sea-surface-temperature)). 

------------------------------------------------------------------------

# Updates

Data was downloaded from CEDA on January 16, 2025.

------------------------------------------------------------------------

# Data Source

**Reference**: Good, S.A.; Embury, O. (2024): ESA Sea Surface Temperature Climate Change Initiative (SST_cci): Level 4 Analysis product, version 3.0. NERC EDS Centre for Environmental Data Analysis, 09 April 2024. doi:10.5285/4a9654136a7148e39b7feb56f8bb02d2. https://dx.doi.org/10.5285/4a9654136a7148e39b7feb56f8bb02d2

**Downloaded**: January 16, 2025

**Description**: ESA Sea Surface Temperature Climate Change Initiative (SST_cci): Level 4 Analysis product, version 3.0

**Time range**: 1980 - ongoing, updated daily at one month behind present

**Format**: NetCDF

**File location**: `Mazu:/home/shares/ohi/OHI_GOC/_raw_data/CEDA_CDRv3.0/19800101120000-ESACCI-L4_GHRSST-SSTdepth-OSTIA-GLOB_CDR3.0-v02.0-fv01.0.nc`

------------------------------------------------------------------------

# Methods

1. Set-up source and file paths

2. Download data needed

3. Process Historical Data (1980 - 1990)

4. Process Current Data (2014 - 2024)

5. Determine anomalies (2sd)


For reference of methods for other spatial pressures we have done (for my use only, delete later):
prs_oa:

<!-- 3.  Split the global OA MultiLayer NetCDF into its individual raster layers, which are by month. -->

<!-- -   This would be saved in Mazu, within `/home/shares/ohi/git-annex/globalprep/prs_oa/v2024/int/oa_monthly_rasters` -->

<!-- 4.  Raster calculations for historical and new data -->

<!-- -   Create a raster of the average historical values by making a `terra` RasterBrick and calculate the average over the reference years (1985 - 2000) -->
<!--     -   Save within `/home/shares/ohi/git-annex/globalprep/prs_oa/v2024/int` -->
<!-- -   Create annual mean rasters for the new data by stacking the monthly rasters by year and using `raster::calc` to calculate the mean for that year. -->
<!--     -   Save within `/home/shares/ohi/git-annex/globalprep/prs_oa/v2024/int/oa_annual_mean` -->

<!-- 5.  Rescale each annual raster between 0 to 1 using the historical average data as a reference -- v2024 updated the function -->

<!-- 6.  Project, resample, and check the extent of the new data, historical ref data, and zones raster from OHI -->

<!-- 7.  Calculate Zonal Statistics using the "mean" between the zones raster and the rescaled annual rasters for each region. -->
<!--     Finish by saving the dataframe within `/home/lecuona/OHI_Intro/ohiprep_v2024/globalprep/prs_oa/v2024/output`. -->

prs_slr:

<!-- - Clips all monthly rasters to the coast using a 3 nautical mile offshore buffer -->
<!-- - Calculates annual mean sea level anomaly rasters from monthly data -->
<!-- - Rescales values from 0 to 1 using the reference point -->
<!-- - Sets to zero all negative values, indicating decreases in mean sea level -->
<!-- - Resamples raster to ~ 1km^2^ and reproject to Molleweide -->
    
------------------------------------------------------------------------

# Setup

```{r}
library(raster)
library(terra)
library(readr)
library(magrittr)
library(mregions2)
library(mapview)
library(sf)
library(tidyverse)
library(lwgeom)
library(here)
library(leaflet.extras2)
library(fasterize)
library(tictoc)
library(kableExtra)
library(purrr)
library(foreach)
library(doParallel)
library(ncdf4)
library(plotly)
library(fs) # for file path handling
library(leaflet)
library(tidyterra)
library(ggspatial)
library(ggplot2)
library(gridExtra)
library(viridis)

# ---- sources! ----
# source(here("workflow", "R", "common.R")) # file creates objects to process data
## set the mazu and neptune data_edit share based on operating system
dir_M             <- c('Windows' = '//mazu.nceas.ucsb.edu/ohi',
                       'Darwin'  = '/Volumes/ohi',    ### connect (cmd-K) to smb://mazu/ohi
                       'Linux'   = '/home/shares/ohi')[[ Sys.info()[['sysname']] ]]

# ---- set year and file path info ----
current_year <- 2025 # Update this in the future!!
version_year <- paste0("v",current_year)
data_dir_version_year <- paste0("d", current_year)

# ---- data directories ----

# raw data directory (on Mazu)
raw_data_dir <- here::here(dir_M, "OHI_GOC", "_raw_data")

# CEDA raw data directory
ceda_dir <- here(raw_data_dir, "CEDA_CDRv3.0", data_dir_version_year)

# output data dir for intermediate data products
int_dir <- here(ceda_dir, "int")
# dir.create(int_dir) # to create the path on Mazu if it has not already been done

# final output dir
output_dir <- here("_pressures", "prs_sst", version_year, "output")

# spatial data for GoC
goc_spatial <- here("spatial")

# set colors
# cols = rev(colorRampPalette(brewer.pal(9, 'Spectral'))(255)) # rainbow color scheme

# this CRS might be better for visualization, explore.
gulf_crs <- "+proj=aea +lat_1=23 +lat_2=30 +lat_0=25 +lon_0=-110 +datum=WGS84 +units=m +no_defs"

# ---------- ecoregion border -----------
goc_ecoregion_border_shp <- st_read(here(goc_spatial, "GoC_polygon_ecoregion_eqarea.shp")) # polygon, proj crs: NAD83 / Conus Albers
```


# Download the data

There are different file naming conventions pre and post 2022.

For pre-2022:

```{r}
# use wget to download all data
base_url <- "https://dap.ceda.ac.uk/neodc/eocis/data/global_and_regional/sea_surface_temperature/CDR_v3/Analysis/L4/v3.0.1"
start_year <- 2001
end_year <- 2013

# set up parallel processing for faster download
cl <- 20
registerDoParallel(cl) # creates a set of copies of R running in parallel and communicating over sockets
# function to generate URLs for each year

download_by_year <- function(year) {
  
  urls <- c() # combine all urls into a list
  
  for (month in 1:12) {
    days_in_month <- if (month == 2) 28 else if (month %in% c(4, 6, 9, 11)) 30 else 31
    for (day in 1:days_in_month) {
      url <- sprintf("%s/%04d/%02d/%02d/%04d%02d%02d120000-ESACCI-L4_GHRSST-SSTdepth-OSTIA-GLOB_CDR3.0-v02.0-fv01.0.nc", 
                     base_url, year, month, day, year, month, day)
      urls <- c(urls, url) # combine that list after every "url" is made
    }
  }
# download the files for each year
  output_dir <- sprintf("/home/shares/ohi/OHI_GOC/_raw_data/CEDA_CDRv3.0/d2025/%04d", year) # mazu output directory
  
  dir.create(output_dir, showWarnings = FALSE, recursive = TRUE) # create the dir if it has not been created yet for that year
  
  wget_command <- sprintf("wget -N -P %s -i -", output_dir) # defining where wget needs to download the data into, and how: -N provides a timestamp so if the download fails, then if a file was halfway downloaded/corrupted it can be determined.  -P %s defines the filepath as output_dir, and -i - tells wget to read the list of urls we made earlier in the function.
  
  system(wget_command, input = paste(urls, collapse = "\n")) # this actually calls wget to work
  
  return(length(urls)) # gives us progress tracking!
  
}
# use the function to process each year in parallel for speed
tic()
results <- foreach(year = start_year:end_year, .combine = c) %dopar% {
  
  download_by_year(year)
  
}
toc() # v2025: took 276.088 sec elapsed for one year (ex: 2014 to 2015)
# double check how many files were downloaded
total_downloaded <- sum(results)
total_downloaded # 730, great! That is for 2 years.  I will work with the first two years to write the scripts to limit processing time, then return back and download all the other years.  It is estimated to take ~5 minutes for a year's worth of data, using 10 cores.
# best way to do it is to go two years at a time (ex: start = 2014, end = 2015; the next download would be start = 2016 to end = 2017).  It is fastest that way.
#### check that the files match the correct dates: restrict the start year and end year to just two years (eg. 1980 - 1981) and see if you get 730 files with names that match the correct the year, month, date is correct as the link in https://dap.ceda.ac.uk/neodc/eocis/data/global_and_regional/sea_surface_temperature/CDR_v3/Analysis/L4/v3.0.1/
```

For 2022 and onwards:

```{r}
# use wget to download all data
base_url <- "https://dap.ceda.ac.uk/neodc/eocis/data/global_and_regional/sea_surface_temperature/CDR_v3/Analysis/L4/v3.0.1"
start_year <- 2024
end_year <- 2024

# set up parallel processing for faster download
cl <- 10
registerDoParallel(cl) # creates a set of copies of R running in parallel and communicating over sockets

# function to generate URLs for each year
download_by_year <- function(year) {
  
  urls <- c() # combine all urls into a list
  
  ## Sophia: I changed the link to be "ICDR3.0" vs. "CDR3.0"
  for (month in 1:12) { #month=1 day=1
    days_in_month <- if (month == 2) 28 else if (month %in% c(4, 6, 9, 11)) 30 else 31
    for (day in 1:days_in_month) {
      url <- sprintf("%s/%04d/%02d/%02d/%04d%02d%02d120000-ESACCI-L4_GHRSST-SSTdepth-OSTIA-GLOB_ICDR3.0-v02.0-fv01.0.nc", 
                     base_url, year, month, day, year, month, day)
      urls <- c(urls, url) # combine that list after every "url" is made
    }
  }

# download the files for each year
  output_dir <- sprintf("/home/shares/ohi/OHI_GOC/_raw_data/CEDA_CDRv3.0/d2025/%04d", year) # mazu output directory
  
  dir.create(output_dir, showWarnings = FALSE, recursive = TRUE) # create the dir if it has not been created yet for that year
  
  wget_command <- sprintf("wget -N -P %s -i -", output_dir) # defining where wget needs to download the data into, and how: -N provides a timestamp so if the download fails, then if a file was halfway downloaded/corrupted it can be determined.  -P %s defines the filepath as output_dir, and -i - tells wget to read the list of urls we made earlier in the function.
  
  system(wget_command, input = paste(urls, collapse = "\n")) # this actually calls wget to work
  
  return(length(urls)) # gives us progress tracking!
  
}

# use the function to process each year in parallel for speed
tic()
results <- foreach(year = start_year:end_year, .combine = c) %dopar% {
  
  download_by_year(year)
  
}
toc() # v2025: took 276.088 sec elapsed for one year (ex: 2014 to 2015)

# double check how many files were downloaded
total_downloaded <- sum(results)
total_downloaded # 730, great! That is for 2 years.  I will work with the first two years to write the scripts to limit processing time, then return back and download all the other years.  It is estimated to take ~5 minutes for a year's worth of data, using 10 cores.

# best way to do it is to go two years at a time (ex: start = 2014, end = 2015; the next download would be start = 2016 to end = 2017).  It is fastest that way.

#### check that the files match the correct dates: restrict the start year and end year to just two years (eg. 1980 - 1981) and see if you get 730 files with names that match the correct the year, month, date is correct as the link in https://dap.ceda.ac.uk/neodc/eocis/data/global_and_regional/sea_surface_temperature/CDR_v3/Analysis/L4/v3.0.1/
```



# Processing Data

## Exploration

Looking at a singular NetCDF file to understand the number of layers

```{r}
# read in NetCDF file downloaded from CEDA
sst_raw <- terra::rast(here(ceda_dir, "1980", "19800101120000-ESACCI-L4_GHRSST-SSTdepth-OSTIA-GLOB_CDR3.0-v02.0-fv01.0.nc")) # S4 class SpatRaster
crs(sst_raw) # not in Albers Eq Area EPSG:5070, will need to be reprojected
plot(sst_raw) # right now all 4 variables are present, each as a separate layer.  However, I want to have it so that there should be a yearly layer for the sea surface temperature. 

# extract the analysed_sst only (perhaps add uncertainty later?)
sst_layer <- sst_raw[["analysed_sst"]]
plot(sst_layer) # now I only see the sst, great!

# split the sst_layer by year using the `terra` package, making it a SpatRasterDataset object
sst_yearly <- terra::split(sst_layer, "years") # now it is a list of 1
# sst_names <- lapply(sst_yearly, magrittr::extract, 1) # using magrittr::extract() instead of '[[]]'
sst_list <- as.list(sst_yearly)

num_layers <- terra::nlyr(sst_layer)
print(num_layers) # just 1, as expected, since we extracted SST

# going line by line of function used for historical and current data to ensure it works properly

# ---------------------------
# how to isolate the analysed SST in one line
analysed_sst <- terra::rast(here(ceda_dir, "1980", "19800101120000-ESACCI-L4_GHRSST-SSTdepth-OSTIA-GLOB_CDR3.0-v02.0-fv01.0.nc"))[["analysed_sst"]]

# reprojecting and clipping
# first, get the CRS string from the ecoregion polygon
target_crs <- st_crs(goc_ecoregion_border_shp)$wkt

# then, reproject the raster using terra
analysed_sst_proj <- terra::project(analysed_sst, target_crs)

# clip the raster and mask in the same line
analyzed_sst_clip <- terra::crop(analysed_sst_proj, goc_ecoregion_border_shp, mask = TRUE)
plot(analyzed_sst_clip) # yay, looks great!

# -----------------------------
# FASTER PROCESSING ORDER:
sst_og_crs <- st_crs(analysed_sst)$wkt
goc_ecoregion_border_sst_proj <- sf::st_transform(goc_ecoregion_border_shp, sst_og_crs)

print(st_bbox(goc_ecoregion_border_sst_proj))
print(ext(analysed_sst))

## clip then reproject to save on processing time
analyzed_sst_clip_first <- terra::crop(analysed_sst, goc_ecoregion_border_sst_proj)
analyzed_sst_reproject_after <- terra::project(analyzed_sst_clip_first, target_crs)
analyzed_sst_mask_last <- terra::mask(analyzed_sst_reproject_after, goc_ecoregion_border_shp)
plot(analyzed_sst_mask_last)
```

## Historical Baseline data (as of now, 1980 - 1990)

### Process yearly average

Original methods for processing:

- Make a list of all files for each year
- Select only the "analysed_sst" variable
- Reproject each daily raster to the same CRS as GoC_polygon_ecoregion_eqarea.shp polygon (NAD83 albers eq area)
- Crop and mask to ensure only SST within the GoC ecoregion boundary is being processed
- Take the mean of all layers for the year to determine the yearly average (in a single layer raster)
- Do this for all years, then combine the single layer rasters into one multilayer SpatRaster, for easy visualization of all years

Processing was taking very long.  So I decided to crop to the rectangle first, then reproject, and mask afterwards.  This sped up the process (along with the use of many cores...).

```{r}
# defining which years to evaluate and process
hist_start_year <- 1991
hist_end_year <- 2000

# bringing this from Setup for ease
goc_ecoregion_border_shp <- st_read(here(goc_spatial, "GoC_polygon_ecoregion_eqarea.shp")) # polygon, proj crs: NAD83 / Conus Albers
analysed_sst <- terra::rast(here(ceda_dir, "1980", "19800101120000-ESACCI-L4_GHRSST-SSTdepth-OSTIA-GLOB_CDR3.0-v02.0-fv01.0.nc"))[["analysed_sst"]]
sst_og_crs <- st_crs(analysed_sst)$wkt
goc_ecoregion_border_sst_proj <- sf::st_transform(goc_ecoregion_border_shp, sst_og_crs)

# define the target crs for all downloaded rasters
target_crs <- st_crs(goc_ecoregion_border_shp)$wkt

# define where each yearly avg raster is saved: historical in_dir in mazu
hist_int_dir <- here(int_dir, "historical_int")
hist_daily_int_dir <- here(hist_int_dir, "hist_daily_int")

# enable terra parallelization
terraOptions(threads = 60, progress = 3, todisk = TRUE, memfrac = 0.5)

hist_process_sst_daily <- function(year, goc_ecoregion_border_sst_proj, goc_ecoregion_border_shp) {
  
  # make a list of all NetCDF files for each year
  files <- fs::dir_ls( # use the `fs` package because it allows us to list files in a more intuitive way
    path = here::here(ceda_dir, as.character(year)),
    glob = "*.nc"
  )
  
  # read all files into a list, selecting only the "analysed_sst" variable
  daily_rasters <- lapply(files, function(f) { # for each iteration, the current file path is passed as the argument `f` to the anonymous function! so it will do this for each daily raster within each year
    
    r <- terra::rast(f)[["analysed_sst"]] # read in each individual .nc file for the year, isolate the analysed_sst layer
    
    r <- terra::crop(r, goc_ecoregion_border_sst_proj, overwrite = TRUE) # makes the maximum extent of the daily raster limited to the goc ecoregion boundary polygon
    
    # reproject the daily raster to match the ecoregion boundary's CRS
    r <- terra::project(r, target_crs) # NAD93, albers eq area
    
    # clip the daily raster to the GoC ecoregion boundary (using crop() and mask(), see <https://r.geocompx.org/raster-vector> for further explanation)
    
    r <- terra::mask(r, goc_ecoregion_border_shp) # replaces all values outside of the NAD83 ecoregion boundary area to NA
    
    # save the daily raster after it has been processed for better progress tracking
    date <- format(as.Date(terra::time(r)), "%Y_%m_%d")
    daily_out_file <- file.path(hist_daily_int_dir, paste0("daily_proj_crop_sst_", date, ".tif"))
    terra::writeRaster(r, filename = daily_out_file, overwrite = TRUE)
    cat(sprintf("Finished processing %s daily raster\n", date))
    
    return(r) # return the reprojected and clipped daily raster
    
  })
  
  # convert all daily rasters for the year to a multilayer SpatRaster
  sst_stack <- terra::rast(daily_rasters) # just reading in a list will make it a multilayer SpatRaster, where each raster in the list becomes a layer
  
  # calculate the average SST for the year
  yearly_avg <- app(sst_stack, fun = mean, na.rm = TRUE) # terra::app applies a function to all layers
  
  # save the processed yearly raster
  out_file <- here(hist_int_dir, paste0("hist_sst_yearly_avg_", year, ".tif"))
  terra::writeRaster(yearly_avg, filename = out_file, overwrite = TRUE)
  
  return(yearly_avg) # returns a single layer raster where each pixel is the average of the SST over all days in the year, saves in Mazu
}

# set up parallel processing using `doParallel`
cl <- 60
registerDoParallel(cl)

# use the function with foreach for faster processing time!
tic()
hist_yearly_averages <- foreach(year = hist_start_year:hist_end_year) %dopar% {
  
  hist_process_sst_daily(year, goc_ecoregion_border_sst_proj, goc_ecoregion_border_shp)
  
}
toc() # for 1980, it took 860.973 sec elapsed
```

Plot the yearly average to verify:

```{r}
test_sst <- terra::rast(here(hist_daily_int_dir, "daily_proj_crop_sst_1986_07_23.tif"))
plot(test_sst)

year_avg_1981 <- terra::rast(here(hist_daily_int_dir, "hist_sst_yearly_avg_1982.tif"))
plot(year_avg_1981)
year_avg_1990 <- terra::rast(here(hist_daily_int_dir, "hist_sst_yearly_avg_1990.tif"))
plot(year_avg_1990)
```

### Bring all the yearly averages into a single multilayer raster

Bring all the daily rasters that have been reprojected and cropped/masked into a multilayer SpatRaster

```{r}
# bring all yearly averages into a single multilayer raster
hist_yearly_sst_list <- fs::dir_ls( # use the `fs` package because it allows us to list files in a more intuitive way
    path = here::here(hist_int_dir),
    glob = "*.tif"
  )

hist_yearly_averages <- terra::rast(hist_yearly_sst_list)
plot(hist_yearly_averages, col=viridis::cividis(n=255))
terra::nlyr(hist_yearly_averages) # 21 layers makes sense since it includes 2000!


# save the multilayer SpatRaster for all historical years to mazu
terra::writeRaster(hist_yearly_averages, filename = here(int_dir, "historical_yearly_sst_averages_goc_eq_area_1980_2000.tif"), overwrite = TRUE)

# # FOR SHINY:
# historical_raster_mask <- terra::mask(hist_yearly_averages_trim, mask = !is.na(hist_yearly_averages_trim))
# 
# terra::writeRaster(historical_raster_mask, filename = here(int_dir, ("trim_mask_historical_yearly_sst_averages_goc_eq_area_1980_2000.tif")), overwrite = TRUE)

# also save it to the data folder within `sst_yearly_comparison_shiny_app` for visualization
# shiny_data_dir <- here()
# terra::writeRaster(hist_yearly_averages, filename = here(int_dir, "historical_yearly_sst_averages_goc_eq_area_1980_2000.tif"), overwrite = TRUE)
```

### Convert from K to Celsius

```{r}
# write a function to convert the raster stack of historical yearly averages from K to C
kelvin_to_celsius <- function(kelvin_temp) {
  
  historical_raster_celcius = kelvin_temp - 273.15
  
  return(historical_raster_celcius)
}

historical_raster_celsius <- kelvin_to_celsius(hist_yearly_averages) # no need for attribute selection since it is a grid-based raster. arithmetic is applied to all cells simultaneously.

plot(historical_raster_celsius)


# check original range of values (in Kelvin)
range(hist_yearly_averages)

# check new range of values (in Celsius)
range(historical_raster_celsius) # great!

# save to mazu
terra::writeRaster(historical_raster_celsius, filename = here(int_dir, "historical_yearly_sst_averages_celsius_goc_eq_area_1980_2000.tif"), overwrite = TRUE)
```


### Process weekly averages

```{r}
# # defining which years to evaluate and process
# hist_start_year <- 1991
# hist_end_year <- 2000
# 
# # directory for daily rasters to be saved in mazu
# hist_weekly_int_dir <- here::here(int_dir, "hist_weekly_int") 
# # dir.create(hist_weekly_int_dir, showWarnings = FALSE, recursive = TRUE)
# 
# # define the target crs for all downloaded rasters
# # target_crs <- st_crs(goc_ecoregion_border_shp)$wkt
# 
# # enable terra parallelization
# terraOptions(threads = 60, progress = 3, todisk = TRUE, memfrac = 0.5)
# 
# # --------------------- process in to weekly rasters -------------
# hist_process_sst_weekly <- function(year) {
#   
#   # make a list of all daily .tif files for historical data
#   files <- fs::dir_ls( 
#     path = here::here(hist_daily_int_dir),
#     glob = "*.tif"
#   )
#   
#   # read in the list as a multilayer SpatRaster with each layer as a day
#   daily_rasters <- terra::rast(files)
#   
#   # add time information to the raster stack (assuming filenames contain dates in YYYY_MM_DD format)
#   dates <- as.Date(sub(".*_(\\d{4}_\\d{2}_\\d{2}).tif", "\\1", basename(files)), "%Y_%m_%d")
#   time(daily_rasters) <- dates
#   
#   # aggregate daily rasters into weekly averages using terra::tapp()
#   weekly_avg <- tapp(daily_rasters, index = "week", fun = mean, na.rm = TRUE)
#   
#   # save each weekly raster as a separate file
#   for (i in seq_len(nlyr(weekly_avg))) {
#     week_start_date <- format(as.Date(names(weekly_avg)[i]), "%Y_%m_%d")
#     output_file <- file.path(hist_weekly_int_dir, paste0("weekly_avg_sst_", week_start_date, ".tif"))
#     terra::writeRaster(weekly_avg[[i]], filename = output_file, overwrite = TRUE)
#     cat(sprintf("Saved weekly raster: %s\n", output_file))
#   }
#   
#   return(weekly_avg) # Return the weekly average raster stack for further analysis if needed
# }
# 
# # set up parallel processing using `doParallel`
# cl <- 60
# registerDoParallel(cl)
# 
# # use the function with foreach for faster processing time!
# tic()
# hist_yearly_averages <- foreach(year = hist_start_year:hist_end_year) %dopar% {
#   
#   hist_process_sst_weekly(year)
#   
# }
# toc() 
```


### Visualizations

```{r}
# quick plotting of layers on Mazu
terra::plot(hist_yearly_averages[[1]], 
            col = viridis::viridis(10), 
            main = "Sea Surface Temperature in 1980")

terra::plot(hist_yearly_averages[[10]], 
            col = viridis::viridis(10), 
            main = "Sea Surface Temperature in 1990")
```


#### Leaflets

Leaflet of only 1980
```{r}
leaf_map <- terra::plet(hist_yearly_averages, main = "SST 1980", tiles = "Streets", col = viridis::cividis(n=255))
leaf_map

# save it to int_dir in mazu: "/home/shares/ohi/OHI_GOC/_raw_data/CEDA_CDRv3.0/d2025/int"
# writeRaster(hist_yearly_averages, filename = here(int_dir, "historical_yearly_sst_averages_goc_eq_area_1980_1990.tif"), overwrite = TRUE)
```


Leaflet of all historical SST, with the ability to add each layer on
```{r}
# create a list of RasterLayer objects (so addRasterImage can be used), one for each year
layer_list <- list()
for (i in 1:nlyr(hist_yearly_averages)) {
  year <- 1979 + i # start in 1980 until 1990
  layer_name <- paste0("SST ", year)
  layer_list[[layer_name]] <- raster(hist_yearly_averages[[i]])
}

# create the base leaflet map
leaf_map <- leaflet() %>%
  addTiles(group = "OpenStreetMap") %>%
  addProviderTiles("Esri.WorldStreetMap", group = "Esri WorldStreetMap")

# add each layer to the map
for (name in names(layer_list)) {
  leaf_map <- leaf_map %>% addRasterImage(layer_list[[name]], colors = colorNumeric(cividis(255), values(layer_list[[name]]), na.color = "transparent"), group = name, opacity = 0.7)
}

# make it so the legend allows the viewer to choose which yearly avg SST raster they would like to see overlayed
leaf_map <- leaf_map %>%
  addLayersControl(
    baseGroups = c("OpenStreetMap", "Esri WorldStreetMap"),
    overlayGroups = names(layer_list),
    options = layersControlOptions(collapsed = FALSE)
  )
leaf_map
```


Interactive leaflet with a time slider... did not work
```{r}
# library(leaflet.extras2)
# 
# # convert SpatRaster to a list of RasterLayer objects, since addTimeslider() and timesliderOptions() do not work with SpatRaster objects.
# hist_raster_list <- list()
# hist_year_range <- 1980:1990
# for(i in 1:nlyr(hist_yearly_averages)) {
#   current_raster <- raster(hist_yearly_averages[[i]])
#   # Create a timestamp for each year (using middle of the year)
#   timestamp <- as.POSIXct(paste0(hist_year_range[i], "-06-15"))
#   hist_raster_list[[as.character(timestamp)]] <- current_raster
# }
# 
# # color palette
# pal <- colorNumeric(palette = "viridis", 
#                    domain = range(values(hist_yearly_averages), na.rm = TRUE))
# 
# # names(hist_yearly_averages) <- paste0("Year_", 1:nlyr(hist_yearly_averages))
# 
# # make a leaflet map with time slider
# leaflet() %>%
#   addTiles() %>%
#   addRasterImage(hist_raster_list[[1]], 
#                  colors = pal, 
#                  opacity = 0.8) %>%
#   addLegend(pal = pal, 
#             values = range(values(hist_yearly_averages), na.rm = TRUE),
#             title = "Temperature (°C)") %>%
#   addTimeslider(
#     data = hist_raster_list,
#     options = timesliderOptions(
#       position = "bottomleft",
#       timeAttribute = "time",
#       range = FALSE,
#       sameDate = TRUE,
#       alwaysShowDate = TRUE
#     )
#   )
```

#### Animation (Historical Data)

YAY it looks great!! It even captures the 1982 - 1983 El Nino that brought warmer waters to the GoC. <https://swfsc-publications.fisheries.noaa.gov/publications/CR/1985/8565.PDF>

```{r}
library(gganimate)

if (nlyr(hist_yearly_averages) == 1) {
  hist_yearly_averages <- c(hist_yearly_averages, hist_yearly_averages)
  names(hist_yearly_averages) <- c("Year_1", "Year_2")
} else {
  names(hist_yearly_averages) <- paste0("Year_", 1:nlyr(hist_yearly_averages))
}

# convert the SpatRaster to a data frame with coordinates and time
hist_sst_df <- as.data.frame(hist_yearly_averages, xy = TRUE) %>%
  tidyr::pivot_longer(
    cols = starts_with("Year"),
    names_to = "year",
    values_to = "temperature"
  ) %>%
  # bring out the year number from the layer name
  mutate(
    year = as.numeric(gsub("Year_", "", year)) + 1979
  )

# make the base plot
hist_sst_plot <- ggplot() +
  # use geom_raster instead of geom_spatraster since we're using a dataframe
  geom_raster(
    data = hist_sst_df,
    aes(x = x, y = y, fill = temperature)
  ) +
  scale_fill_viridis_c(
    option = "mako",
    name = "Temperature (K)",
    na.value = NA
  ) +
  coord_sf() +  # use coord_sf instead of coord_equal, was having issues earlier
  theme_minimal() +
  labs(
    title = 'Sea Surface Temperature {closest_state}',
    x = "Longitude",
    y = "Latitude"
  ) +
  theme(
    plot.title = element_text(size = 16, hjust = 0.5),
    legend.position = "right",
    panel.grid.major = element_line(color = "white", size = 0.2),
    panel.grid.minor = element_line(color = "white", size = 0.1)
  )

# make the animation for historical years (1980 - 1990)
hist_sst_animation <- hist_sst_plot +
  transition_states(
    year,
    transition_length = 2,
    state_length = 1
  ) +
  enter_fade() +
  exit_fade() +
  ease_aes('linear')

# save the animation
anim_save(
  filename = here(output_dir, "hist_sst_animation.gif"),
  animation = animate(
    hist_sst_animation,
    nframes = 500,  # more frames = smoother animation
    fps = 10,      # frame rate per second
    width = 800,
    height = 600,
    renderer = gifski_renderer()
  )
) # YAY it looks great!
```

Animation that did not work:
```{r}
# ---------------------
# library(tidyterra)
# library(gganimate)
# library(gifski)
# 
# hist_plots <- lapply(seq_along(hist_yearly_averages), function(i) {
#   ggplot() +
#     geom_spatraster(data = hist_yearly_averages[[i]]) +
#     scale_fill_viridis_c(option = "cividis", name = "Temperature") +
#     labs(title = paste("Sea Surface Temperature -", hist_start_year + i - 1)) +
#     theme_minimal()
# })
# 
# # great a gif from the files...
# sst_anim <- hist_plots %>%
#   gganimate::transition_manual(frames = seq_along(hist_yearly_averages)) +
#   enter_fade() +
#   exit_fade()
# 
# animate(sst_anim, nframes = length(hist_yearly_averages), fps = 1)
# 
# # save animation
# # anim_save("sea_surface_temp.gif")
# 
# # ------------------
# # using ggmap instead
# library(ggmap)
# 
# bbox_ecoregion <- st_bbox(goc_ecoregion_border_sst_proj)
# bbox_ecoregion
# 
# hist_layer_1980 <-  hist_yearly_averages[[1]]
# 
# api_key <- print("03b0b66a-9415-4113-92fa-16b79f4597a6") # made a free account with stadiamaps
# 
# register_stadiamaps(api_key, write = FALSE)
# 
# historical_stamen <- ggmap::get_stadiamap(
#   bbox = c(-114.9427, 20.4068, -105.1876, 31.8236),
#   zoom = 10,
#   maptype = "outdoors"
# )
# 
# ggmap::ggmap(
#   historical_stamen
# ) +
#   tidyterra:: geom_spatraster(
#     data = hist_layer_1980
#   ) +
#   scale_fill_gradientn(
#     colors = hcl.colors(
#       20, palette = "viridis", alpha = 0.8
#     ),
#     na.value = NA
#   ) +
#   theme_void() # does not look that great...
```




## Current SST GoC data (2014 - 2023)

Process current SST daily rasters:

```{r}
# defining which years to evaluate and process
# v2025: does not have a full year of v2024, so it will not be evaluated.
current_start_year <- 2001
current_end_year <- 2013

# bringing this from Setup for ease
goc_ecoregion_border_shp <- st_read(here(goc_spatial, "GoC_polygon_ecoregion_eqarea.shp")) # polygon, proj crs: NAD83 / Conus Albers

# define the target crs for all downloaded rasters
target_crs <- st_crs(goc_ecoregion_border_shp)$wkt

# define where each yearly avg raster is saved: historical in_dir in mazu
current_int_dir <- here(int_dir, "current_int")
current_daily_int_dir <- here(current_int_dir, "current_daily_int")

# enable terra parallelization
terraOptions(threads = 40, progress = 3, todisk = TRUE, memfrac = 0.5)

current_process_sst_daily <- function(year, goc_ecoregion_border_sst_proj, goc_ecoregion_border_shp) {
  
  # make a list of all NetCDF files for each year
  files <- fs::dir_ls( # use the `fs` package because it allows us to list files in a more intuitive way
    path = here::here(ceda_dir, as.character(year)),
    glob = "*.nc"
  )
  
  # read all files into a list, selecting only the "analysed_sst" variable
  daily_rasters <- lapply(files, function(f) { # for each iteration, the current file path is passed as the argument `f` to the anonymous function! so it will do this for each daily raster within each year
    
    r <- terra::rast(f)[["analysed_sst"]] # read in each individual .nc file for the year, isolate the analysed_sst layer
    
    r <- terra::crop(r, goc_ecoregion_border_sst_proj, overwrite = TRUE) # makes the maximum extent of the daily raster limited to the goc ecoregion boundary polygon
    
    # reproject the daily raster to match the ecoregion boundary's CRS
    r <- terra::project(r, target_crs) # NAD93, albers eq area
    
    # clip the daily raster to the GoC ecoregion boundary (using crop() and mask(), see <https://r.geocompx.org/raster-vector> for further explanation)
    
    r <- terra::mask(r, goc_ecoregion_border_shp) # replaces all values outside of the NAD83 ecoregion boundary area to NA
    
    # save the daily raster after it has been processed for better progress tracking
    date <- format(as.Date(terra::time(r)), "%Y_%m_%d")
    daily_out_file <- file.path(current_daily_int_dir, paste0("daily_proj_crop_sst_", date, ".tif"))
    terra::writeRaster(r, filename = daily_out_file, overwrite = TRUE)
    cat(sprintf("Finished processing %s daily raster\n", date))
    
    return(r) # return the reprojected and clipped daily raster
    
  })
  
  # convert all daily rasters for the year to a multilayer SpatRaster
  sst_stack <- terra::rast(daily_rasters) # just reading in a list will make it a multilayer SpatRaster, where each raster in the list becomes a layer
  
  # calculate the average SST for the year
  yearly_avg <- app(sst_stack, fun = mean, na.rm = TRUE) # terra::app applies a function to all layers
  
  # save the processed yearly raster
  out_file <- here(current_int_dir, paste0("current_sst_yearly_avg_", year, ".tif"))
  terra::writeRaster(yearly_avg, filename = out_file, overwrite = TRUE)
  
  return(yearly_avg) # returns a single layer raster where each pixel is the average of the SST over all days in the year, saves in Mazu
}

# set up parallel processing using `doParallel`
cl <- 40
registerDoParallel(cl)

# use the function with foreach for faster processing time!
tic()
current_yearly_averages <- foreach(year = current_start_year:current_end_year) %dopar% {
  
  current_process_sst_daily(year, goc_ecoregion_border_sst_proj, goc_ecoregion_border_shp)
  
}
toc() # for 1980, it took 860.973 sec elapsed
```

### Bring all the daily rasters that have been reprojected and cropped/masked into a multilayer SpatRaster

```{r}
# define where each yearly avg raster is saved: historical in_dir in mazu
current_int_dir <- here(int_dir, "current_int")
current_daily_int_dir <- here(current_int_dir, "current_daily_int")

# bring all yearly averages into a single multilayer raster
current_yearly_sst_list <- fs::dir_ls( # use the `fs` package because it allows us to list files in a more intuitive way
    path = here::here(current_int_dir),
    glob = "*.tif"
  ) %>%
  # exclude 2024 because it is not a full year by excluding files containing "2024" in their names
  purrr::discard(~ grepl("2024", .))

current_yearly_averages <- terra::rast(current_yearly_sst_list)
terra::nlyr(current_yearly_averages) # 23, that is great because it now goes from 2001 - 2023 (for visualization purposes)
plot(current_yearly_averages)

# save it to int_dir in mazu: "/home/shares/ohi/OHI_GOC/_raw_data/CEDA_CDRv3.0/d2025/int"
# writeRaster(current_yearly_averages, filename = here(int_dir, "current_yearly_sst_averages_goc_eq_area_2014_2024.tif"), overwrite = TRUE)

# save the multilayer SpatRaster for all current years to mazu
terra::writeRaster(current_yearly_averages, filename = here(int_dir, paste0("current_yearly_sst_averages_goc_eq_area_2001_2023.tif")), overwrite = TRUE)
```

### Convert from K to Celsius

```{r}
# bring the function over in case this is the only chunk that needs to be run
kelvin_to_celsius <- function(kelvin_temp) {
  
  historical_raster_celcius = kelvin_temp - 273.15
  
  return(historical_raster_celcius)
}

current_raster_celsius <- kelvin_to_celsius(current_yearly_averages) # no need for attribute selection since it is a grid-based raster. arithmetic is applied to all cells simultaneously.

plot(current_raster_celsius)


# check original range of values (in Kelvin)
range(current_yearly_averages)

# check new range of values (in Celsius)
range(current_raster_celsius) # great!

# save to mazu
terra::writeRaster(current_raster_celsius, filename = here(int_dir, "current_yearly_sst_averages_celsius_goc_eq_area_2001_2023.tif"), overwrite = TRUE)
```



### Visualizations

#### Animation (Current)
```{r}
library(gganimate)

if (nlyr(current_yearly_averages) == 1) {
  current_yearly_averages <- c(current_yearly_averages, current_yearly_averages)
  names(current_yearly_averages) <- c("Year_1", "Year_2")
} else {
  names(current_yearly_averages) <- paste0("Year_", 1:nlyr(current_yearly_averages))
}

# convert the SpatRaster to a data frame with coordinates and time
current_sst_df <- as.data.frame(current_yearly_averages, xy = TRUE) %>%
  tidyr::pivot_longer(
    cols = starts_with("Year"),
    names_to = "year",
    values_to = "temperature"
  ) %>%
  # bring out the year number from the layer name
  mutate(
    year = as.numeric(gsub("Year_", "", year)) + 2013
  )

# make the base plot
current_sst_plot <- ggplot() +
  # use geom_raster instead of geom_spatraster since we're using a dataframe
  geom_raster(
    data = current_sst_df,
    aes(x = x, y = y, fill = temperature)
  ) +
  scale_fill_viridis_c(
    option = "mako",
    name = "Temperature (K)",
    na.value = NA
  ) +
  coord_sf() +
  theme_minimal() +
  labs(
    title = 'Sea Surface Temperature {closest_state}',
    x = "Longitude",
    y = "Latitude"
  ) +
  theme(
    plot.title = element_text(size = 16, hjust = 0.5),
    legend.position = "right",
    panel.grid.major = element_line(color = "white", size = 0.2),
    panel.grid.minor = element_line(color = "white", size = 0.1)
  )

# make the animation for historical years (1980 - 1990)
current_sst_animation <- current_sst_plot +
  transition_states(
    year,
    transition_length = 2,
    state_length = 1
  ) +
  enter_fade() +
  exit_fade() +
  ease_aes('linear')

# save the current data animation
anim_save(
  filename = here(output_dir, "current_sst_animation.gif"),
  animation = animate(
    current_sst_animation,
    nframes = 500,  # more frames = smoother animation
    fps = 20,      # frame rate per second
    width = 800,
    height = 600,
    renderer = gifski_renderer()
  )
)
```
```{r}
# Remove outer rows/columns with only NA values
historical_raster <- terra::trim(historical_raster)
current_raster <- terra::trim(current_raster)

# Alternatively, mask NA values explicitly (if you have a mask layer or want to set a specific value)
historical_raster <- terra::mask(historical_raster, mask = !is.na(historical_raster))
current_raster <- terra::mask(current_raster, mask = !is.na(current_raster))
```



# Full 1984 - 2023 raster

This is a combined raster of all years, 1984 - 2023.

```{r}
# check they have the same extent
setdiff(terra::ext(current_raster_celsius), terra::ext(historical_raster_celsius)) # same extent

# check they have the same resolution
setdiff(terra::res(current_raster_celsius), terra::res(historical_raster_celsius)) # same resolution

# check they have the same CRS
identical(terra::crs(historical_raster_celsius), terra::crs(current_raster_celsius)) # TRUE

# confirm alignment
aligned <- terra::compareGeom(historical_raster_celsius, current_raster_celsius, stopOnError = FALSE)
print(aligned) # returns TRUE, which is good

# ====== looks good, lets combine! ===========
# list of files from the current data directory
current_yearly_sst_list <- fs::dir_ls(
  path = here::here(current_int_dir),
  glob = "*.tif"
) %>%
  purrr::discard(~ grepl("2024", .)) # Exclude files containing "2024"

# list of files from the historical data directory
hist_yearly_sst_list <- fs::dir_ls(
  path = here::here(hist_int_dir),
  glob = "*.tif"
)

# combine both lists into a single list
combined_yearly_sst_list <- c(current_yearly_sst_list, hist_yearly_sst_list)

# make into a multilayer SpatRaster
all_years_avg_sst <- terra::rast(combined_yearly_sst_list)
plot(all_years_avg_sst)
terra::nlyr(all_years_avg_sst) # 44, yay! That makes sense because it is 21 + 23

# ======= convert to Celsius from K ========
kelvin_to_celsius <- function(kelvin_temp) {
  
  all_years_avg_sst = kelvin_temp - 273.15
  
  return(all_years_avg_sst)
}

all_years_avg_sst_celsius <- kelvin_to_celsius(all_years_avg_sst) 

range(all_years_avg_sst_celsius)
plot(all_years_avg_sst_celsius)

# save to mazu
terra::writeRaster(all_years_avg_sst_celsius, filename = here(int_dir, "all_yearly_sst_averages_celsius_goc_eq_area_1980_2023.tif"), overwrite = TRUE)
```



# Yearly comparisons raster

Here, we will be making a raster of all current years (2014 - 2023) subtracted by a historically averaged SST raster (1980 - 2000). That way, we can see which years have a greater difference 

Idea: make this a different color scale to show that it is a different type of visualization.  Perhaps using red of some sort.


## Bring in historical and current rasters

### Historical baseline average

```{r}
# bring in historical stacked raster in celsius
historical_baseline <- terra::rast(here(int_dir, "historical_yearly_sst_averages_celsius_goc_eq_area_1980_2000.tif"))
terra::nlyr(historical_baseline) # 21, as expected

# average all years (1980 - 2000) into one layer
historical_baseline_avg <- terra::app(historical_baseline, fun = mean, na.rm = TRUE)
terra::nlyr(historical_baseline_avg) # 1, as expected!

plot(historical_baseline_avg)
```

### Current SST yearly averages

```{r}
# bring in current stacked raster in celsius
current_yearly_rast <- terra::rast(here(int_dir, "current_yearly_sst_averages_celsius_goc_eq_area_2001_2023.tif"))
terra::nlyr(current_yearly_rast) # 23, as expected
```

## Subtract current layers each by the historical baseline

```{r}
comparison_current_hist_baseline <- current_yearly_rast - historical_baseline_avg
terra::nlyr(comparison_current_hist_baseline) # 23, great!

plot(comparison_current_hist_baseline)

# save to mazu
terra::writeRaster(comparison_current_hist_baseline, filename = here(int_dir, "comparison_current_hist_baseline_celsius_goc_eq_area_2001_2023.tif"), overwrite = TRUE)
```


