---
title: 'OHI GoC 2024: Sea Surface Temperature (SST)'
author: "*Compiled on `r date()` by `r Sys.info()['user']`*"
format:
  html:
    code-fold: show
    toc: true
    toc-depth: 3
    toc-float: true
    number-sections: false
    theme: cerulean
    highlight-style: haddock
  pdf:
    toc: true
editor: 
  markdown:
    wrap: sentence
  chunk_output_type: inline
editor_options: 
  chunk_output_type: console
---

# Summary

This is a work in progress, aiming to provide a pressure layer of Sea Surface Temperature (SST) for the Gulf of California. We will begin by visualizing the data and determining the yearly average temperature, then progress to a weekly average temperature. We can use the standard deviation to define an anomaly, as was done for OHI global.  The frequency of anomalies in the last 5 years compared to the baseline SST anomalies from historical data will give an idea of how much change has occurred.  Rescaling was done using the “99.99th quantile of raster values from all years”, which we can later evaluate if it should be the same here ([sea-surface-temperature](https://ohi-science.org/ohi-methods/data-layers/data-layer-descriptions.html#sea-surface-temperature)). 

------------------------------------------------------------------------

# Updates

Data was downloaded from CEDA on January 16, 2025.

------------------------------------------------------------------------

# Data Source

**Reference**: Good, S.A.; Embury, O. (2024): ESA Sea Surface Temperature Climate Change Initiative (SST_cci): Level 4 Analysis product, version 3.0. NERC EDS Centre for Environmental Data Analysis, 09 April 2024. doi:10.5285/4a9654136a7148e39b7feb56f8bb02d2. https://dx.doi.org/10.5285/4a9654136a7148e39b7feb56f8bb02d2

**Downloaded**: January 16, 2025

**Description**: ESA Sea Surface Temperature Climate Change Initiative (SST_cci): Level 4 Analysis product, version 3.0

**Time range**: 1980 - ongoing, updated daily at one month behind present

**Format**: NetCDF

**File location**: `Mazu:/home/shares/ohi/OHI_GOC/_raw_data/CEDA_CDRv3.0/19800101120000-ESACCI-L4_GHRSST-SSTdepth-OSTIA-GLOB_CDR3.0-v02.0-fv01.0.nc`

------------------------------------------------------------------------

# Methods

1.  Set-up source and file paths

2.  Download data needed

prs_oa:

<!-- 3.  Split the global OA MultiLayer NetCDF into its individual raster layers, which are by month. -->

<!-- -   This would be saved in Mazu, within `/home/shares/ohi/git-annex/globalprep/prs_oa/v2024/int/oa_monthly_rasters` -->

<!-- 4.  Raster calculations for historical and new data -->

<!-- -   Create a raster of the average historical values by making a `terra` RasterBrick and calculate the average over the reference years (1985 - 2000) -->
<!--     -   Save within `/home/shares/ohi/git-annex/globalprep/prs_oa/v2024/int` -->
<!-- -   Create annual mean rasters for the new data by stacking the monthly rasters by year and using `raster::calc` to calculate the mean for that year. -->
<!--     -   Save within `/home/shares/ohi/git-annex/globalprep/prs_oa/v2024/int/oa_annual_mean` -->

<!-- 5.  Rescale each annual raster between 0 to 1 using the historical average data as a reference -- v2024 updated the function -->

<!-- 6.  Project, resample, and check the extent of the new data, historical ref data, and zones raster from OHI -->

<!-- 7.  Calculate Zonal Statistics using the "mean" between the zones raster and the rescaled annual rasters for each region. -->
<!--     Finish by saving the dataframe within `/home/lecuona/OHI_Intro/ohiprep_v2024/globalprep/prs_oa/v2024/output`. -->

prs_slr:

<!-- - Clips all monthly rasters to the coast using a 3 nautical mile offshore buffer -->
<!-- - Calculates annual mean sea level anomaly rasters from monthly data -->
<!-- - Rescales values from 0 to 1 using the reference point -->
<!-- - Sets to zero all negative values, indicating decreases in mean sea level -->
<!-- - Resamples raster to ~ 1km^2^ and reproject to Molleweide -->
    
------------------------------------------------------------------------

# Setup

```{r}
library(raster)
library(terra)
library(readr)
library(magrittr)
library(mregions2)
library(mapview)
library(sf)
library(tidyverse)
library(lwgeom)
library(here)
library(leaflet.extras2)
library(fasterize)
library(tictoc)
library(kableExtra)
library(purrr)
library(foreach)
library(doParallel)
library(ncdf4)
library(plotly)

# ---- sources! ----
# source(here("workflow", "R", "common.R")) # file creates objects to process data
## set the mazu and neptune data_edit share based on operating system
dir_M             <- c('Windows' = '//mazu.nceas.ucsb.edu/ohi',
                       'Darwin'  = '/Volumes/ohi',    ### connect (cmd-K) to smb://mazu/ohi
                       'Linux'   = '/home/shares/ohi')[[ Sys.info()[['sysname']] ]]

# ---- set year and file path info ----
current_year <- 2025 # Update this in the future!!
version_year <- paste0("v",current_year)
data_dir_version_year <- paste0("d", current_year)

# ---- data directories ----

# raw data directory (on Mazu)
raw_data_dir <- here::here(dir_M, "OHI_GOC", "_raw_data")

# CEDA raw data directory
ceda_dir <- here(raw_data_dir, "CEDA_CDRv3.0", data_dir_version_year)

# output data dir for intermediate data products
int_dir <- here(ceda_dir, "int")
# dir.create(int_dir) # to create the path on Mazu if it has not already been done

# final output dir
output_dir <- here("_pressures", "prs_sst", version_year, "output")

# spatial data for GoC
goc_spatial <- here("spatial")

# set colors
# cols = rev(colorRampPalette(brewer.pal(9, 'Spectral'))(255)) # rainbow color scheme

# this CRS might be better for visualization, explore.
gulf_crs <- "+proj=aea +lat_1=23 +lat_2=30 +lat_0=25 +lon_0=-110 +datum=WGS84 +units=m +no_defs"

# ---------- ecoregion border -----------
goc_ecoregion_border_shp <- st_read(here(goc_spatial, "GoC_polygon_ecoregion_eqarea.shp")) # polygon, proj crs: NAD83 / Conus Albers
```
# Download the data

```{r}
# use wget2 to download all data
base_url <- "https://dap.ceda.ac.uk/neodc/eocis/data/global_and_regional/sea_surface_temperature/CDR_v3/Analysis/L4/v3.0.1"
start_year <- 2022
end_year <- 2023

# set up parallel processing for faster download
cl <- 10
registerDoParallel(cl) # creates a set of copies of R running in parallel and communicating over sockets

# function to generate URLs for each year
process_year <- function(year) {
  
  urls <- c() # combine all urls into a list
  
  for (month in 1:12) {
    days_in_month <- if (month == 2) 28 else if (month %in% c(4, 6, 9, 11)) 30 else 31
    for (day in 1:days_in_month) {
      url <- sprintf("%s/%04d/%02d/%02d/%04d%02d%02d120000-ESACCI-L4_GHRSST-SSTdepth-OSTIA-GLOB_CDR3.0-v02.0-fv01.0.nc", 
                     base_url, year, month, day, year, month, day)
      urls <- c(urls, url) # combine that list after every "url" is made
    }
  }

# download the files for each year
  output_dir <- sprintf("/home/shares/ohi/OHI_GOC/_raw_data/CEDA_CDRv3.0/d2025/%04d", year) # mazu output directory
  
  dir.create(output_dir, showWarnings = FALSE, recursive = TRUE) # create the dir if it has not been created yet for that year
  
  wget_command <- sprintf("wget -N -P %s -i -", output_dir) # defining where wget needs to download the data into, and how: -N provides a timestamp so if the download fails, then if a file was halfway downloaded/corrupted it can be determined.  -P %s defines the filepath as output_dir, and -i - tells wget to read the list of urls we made earlier in the function.
  
  system(wget_command, input = paste(urls, collapse = "\n")) # this actually calls wget to work
  
  return(length(urls)) # gives us progress tracking!
  
}

# use the function to process each year in parallel for speed
tic()
results <- foreach(year = start_year:end_year, .combine = c) %dopar% {
  
  process_year(year)
  
}
toc() # v2025: took 276.088 sec elapsed for one year (ex: 2014 to 2015)

# double check how many files were downloaded
total_downloaded <- sum(results)
total_downloaded # 730, great! That is for 2 years.  I will work with the first two years to write the scripts to limit processing time, then return back and download all the other years.  It is estimated to take ~5 minutes for a year's worth of data, using 10 cores.

# best way to do it is to go two years at a time (ex: start = 2014, end = 2015; the next download would be start = 2016 to end = 2017).  It is fastest that way.

#### check that the files match the correct dates: restrict the start year and end year to just two years (eg. 1980 - 1981) and see if you get 730 files with names that match the correct the year, month, date is correct as the link in https://dap.ceda.ac.uk/neodc/eocis/data/global_and_regional/sea_surface_temperature/CDR_v3/Analysis/L4/v3.0.1/
```



# Read in Data

```{r}
# read in NetCDF file downloaded from CEDA
sst_raw <- terra::rast(here(ceda_dir, "19800101120000-ESACCI-L4_GHRSST-SSTdepth-OSTIA-GLOB_CDR3.0-v02.0-fv01.0.nc")) # S4 class SpatRaster
crs(sst_raw) # not in Albers Eq Area EPSG:5070, will need to be reprojected
plot(sst_raw) # right now all 4 variables are present, each as a separate layer.  However, I want to have it so that there should be a yearly layer for the sea surface temperature. 

# extract the analysed_sst only (perhaps add uncertainty later?)
sst_layer <- sst_raw[["analysed_sst"]]
plot(sst_layer) # now I only see the sst, great!

# split the sst_layer by year using the `terra` package, making it a SpatRasterDataset object
sst_yearly <- terra::split(sst_layer, "years") # now it is a list of 1
# sst_names <- lapply(sst_yearly, magrittr::extract, 1) # using magrittr::extract() instead of '[[]]'
sst_list <- as.list(sst_yearly)

num_layers <- terra::nlyr(sst_layer)
print(num_layers) # v2024: 156 layers, which makes sense because the data goes from 2010 - 2022 monthly

sst_2023 <- sst_list$X2023
```


```{r}
crs(goc_ecoregion_border_shp)
```





